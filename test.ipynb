{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from soc_api import Reddit\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_id = ''\n",
    "client_secret = ''\n",
    "\n",
    "red = Reddit(client_id,client_secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_ids = [p for p in red.get_submissions('MLS',time_filter='week', limit=1000)]\n",
    "print(len(post_ids))\n",
    "post_ids[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread_urls = ['https://www.reddit.com/r/MLS/comments/138w5p7/lizzy_becherano_exclusive_sources_confirm_a_new/',\n",
    "              'https://www.reddit.com/r/MLS/comments/138ts1p/tannenwald_the_full_transcript_of_the_exchange/']\n",
    "\n",
    "coms = red.process_submissions(thread_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = [c['created_date'] for c in coms]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# filename = 'MLS_{}.csv'.format('_'.join((min(dates).strftime(format='%Y-%m-%d'), max(dates).strftime(format='%Y-%m-%d'))))\n",
    "# red.write_csv(coms,filename)\n",
    "coms_df = pd.read_csv('MLS_2023-04-30_2023-05-07.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coms_df = pd.DataFrame(coms)\n",
    "coms_df['score_percent'] = coms_df.score/coms_df.score.sum()\n",
    "print(coms_df.shape)\n",
    "coms_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "pd.Series([a[0] for a in coms_df.parent_id.str.split('_').dropna()]).value_counts()b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coms_df.type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coms_df.url_domain.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coms_df[coms_df.score>2].score.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coms_df[coms_df.score>2].score.hist(by=coms_df['type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coms_df \\\n",
    "    .groupby([pd.to_datetime(coms_df.created_date).dt.date,'type']).count()['id'].reset_index() \\\n",
    "    .pivot(index='created_date', columns='type', values='id') \\\n",
    "    .plot.bar(subplots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coms_df \\\n",
    "    .groupby([pd.to_datetime(coms_df.created_date).dt.date,'type']).sum()['score'].reset_index() \\\n",
    "    .pivot(index='created_date', columns='type', values='score') \\\n",
    "    .plot.bar(subplots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coms_df.gilded.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coms_df[coms_df.controversiality !=0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coms_df.groupby('retrieve_time').count().head()['id']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = red.session# auth\n",
    "posts = reddit.subreddit('MLS').top(time_filter='day', limit=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = [p for p in posts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps[0].id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Model API - Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "databricks/dolly-v2-3b :  [{'generated_text': 'What are you?\\n\\nI am a data scientist and machine learning enthusiast. I have been'}]\n",
      "gpt-3.5-turbo :  {'error': 'Model gpt-3.5-turbo does not exist'}\n",
      "google/flan-t5-large :  [{'generated_text': 'a sailor'}]\n",
      "bigscience/bloomz :  [{'generated_text': 'What are you? a film'}]\n"
     ]
    }
   ],
   "source": [
    "def ask_hf_llm(q,m,token):\n",
    "    '''\n",
    "    Query a hugging face hosted llm model.\n",
    "    '''\n",
    "    import requests\n",
    "\n",
    "    API_URL = \"https://api-inference.huggingface.co/models/{}\".format(m)\n",
    "    headers = {\"Authorization\": \"Bearer {}\".format(token)}\n",
    "\n",
    "    def query(payload):\n",
    "        response = requests.post(API_URL, headers=headers, json=payload)\n",
    "        return response.json()\n",
    "\n",
    "    output = query({\n",
    "        \"inputs\": q,\n",
    "    })\n",
    "    return output\n",
    "\n",
    "q = 'What are you?'\n",
    "for m in [\"databricks/dolly-v2-3b\", \"gpt-3.5-turbo\", \"google/flan-t5-large\",\"bigscience/bloomz\"]:\n",
    "    print(m,': ', ask_hf_llm(q,m,'xxx'))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# LLM Model API - Langchain\n",
    "\n",
    "For huggingface hosted models only supports [`text2text generation`](https://huggingface.co/models?pipeline_tag=text2text-generation&sort=downloads) and [`textgeneration`](https://huggingface.co/models?pipeline_tag=text-generation&sort=downloads) types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import os \n",
    " \n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = ''\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised APIConnectionError: Error communicating with OpenAI: ('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None)).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who is the best soccer player in history? \n",
      "\n",
      "# Dolly:\n",
      " The answer is complicated. There are many ways to measure the best soccer player in history.\n",
      "\n",
      "One way is to compare the number of goals scored. The all-time top scorer is Lionel Messi. He has scored 634 goals in\n",
      "\n",
      "# gpt-3.5-turbo:\n",
      " As an AI language model, I cannot have personal opinions, but according to many experts and fans around the world, the best soccer player in history is widely regarded as Pel√©, Lionel Messi, or Diego Maradona. However, this is a subjective topic and opinions may vary.\n",
      "\n",
      "# Flan:\n",
      " gertrude bell\n",
      "\n",
      "# Bloom:\n",
      "  Pele\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate, HuggingFaceHub, LLMChain\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "# load a few models\n",
    "model_params = {\"temperature\":0, \"max_length\":64}\n",
    "# generate_text = pipeline(model=\"databricks/dolly-v2-3b\", torch_dtype=torch.bfloat16,\n",
    "#                          trust_remote_code=True, device_map=\"auto\", return_full_text=True)\n",
    "# llm_dolly_local = HuggingFacePipeline(pipeline=generate_text) # locally saved dolly pipeline\n",
    "llm_dolly = HuggingFaceHub(repo_id=\"databricks/dolly-v2-3b\", model_kwargs=model_params) # hf hosted llm\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\") # openai hosted llm\n",
    "llm_flan = HuggingFaceHub(repo_id=\"google/flan-t5-large\", model_kwargs=model_params) # hf hosted llm\n",
    "llm_bloom = HuggingFaceHub(repo_id=\"bigscience/bloomz\") # hf hosted llm\n",
    "\n",
    "# define the prompt\n",
    "template = \"\"\" Question: {question}\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "\n",
    "# create chains\n",
    "llm_dolly_chain = LLMChain(prompt=prompt,llm=llm_dolly)\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "llm_chain_flan = LLMChain(prompt=prompt, llm=llm_flan)\n",
    "llm_chain_bloom = LLMChain(prompt=prompt, llm=llm_bloom)\n",
    "\n",
    "# run\n",
    "question = \"Who is the best soccer player in history?\"\n",
    "\n",
    "print(question,'\\n')\n",
    "print('# Dolly:\\n',llm_dolly_chain.run(question).strip())\n",
    "print('\\n# gpt-3.5-turbo:\\n',llm_chain.run(question))\n",
    "print('\\n# Flan:\\n',llm_chain_flan.run(question))\n",
    "print('\\n# Bloom:\\n',llm_chain_bloom.run(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas DataFrame Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: We can use the `shape` attribute of the dataframe to find the number of rows and columns.\n",
      "Action: python_repl_ast\n",
      "Action Input: df.shape\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m(9715, 24)\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThe dataframe has 9715 rows and 24 columns.\n",
      "Final Answer: There are 9715 rows and 24 columns.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'There are 9715 rows and 24 columns.'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents import create_pandas_dataframe_agent\n",
    "\n",
    "df = pd.read_csv('simulated.csv')\n",
    "agent = create_pandas_dataframe_agent(llm, coms_df, verbose=True)\n",
    "\n",
    "agent.run(\"how many rows and columns are there?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "agent.run(\"\"\"Use python_repl_ast tool. Create a TFIDF matrix using the body column and cluster into topics. Convert body column to lowercase.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\twnba\\Anaconda3\\envs\\llm\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "df = coms_df\n",
    "\n",
    "# Remove any rows with NaN values in the body column\n",
    "df.dropna(subset=['body'], inplace=True)\n",
    "\n",
    "# Convert text to lowercase\n",
    "df['body'] = df['body'].str.lower()\n",
    "\n",
    "# Remove punctuation\n",
    "df['body'] = df['body'].str.replace('[^\\w\\s]','')\n",
    "\n",
    "# Remove numbers\n",
    "df['body'] = df['body'].str.replace('\\d+', '')\n",
    "\n",
    "# Remove stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['body'] = df['body'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
    "\n",
    "# Create TFIDF matrix\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf = tfidf_vectorizer.fit_transform(df['body'])\n",
    "\n",
    "# Cluster into topics\n",
    "kmeans = KMeans(n_clusters=5).fit(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    6332\n",
       "4    1745\n",
       "2     628\n",
       "3     450\n",
       "0     380\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(kmeans.labels_).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Dataframe Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Topic Modelling (Clustering)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "coms_df['content'] = coms_df.title.fillna('') + coms_df.submission_text.fillna('') + coms_df.body.fillna('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.document_loaders import DataFrameLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "\n",
    "hf_embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "coms_df['source'] = coms_df.id\n",
    "\n",
    "loader = DataFrameLoader(coms_df, page_content_column=\"content\")\n",
    "docs = loader.load()\n",
    "text_split = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 0)\n",
    "text = text_split.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='[meme] It would end the pain', metadata={'subreddit': 'MLS', 'submission_text': nan, 'created_utc': 1682945845.0, 'controversiality': nan, 'submission_id': '134ls7m', 'score': 1109, 'likes': nan, 'distinguished': nan, 'downs': nan, 'title': '[meme] It would end the pain', 'author': 'Fraganade', 'upvote_ratio': 0.98, 'url_domain': 'i.imgur.com', 'permalink': '/r/MLS/comments/134ls7m/meme_it_would_end_the_pain/', 'retrieve_time': '2023-05-06 23:17:30', 'created_date': '2023-05-01 12:57:25', 'parent_id': nan, 'type': 'submission', 'gilded': nan, 'url': 'https://i.imgur.com/duojFS6.jpg', 'body': nan, 'num_reports': nan, 'total_awards_received': nan, 'id': '134ls7m', 'source': '134ls7m'})"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9845"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9715, 26)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coms_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pass the text and embeddings to FAISS\n",
    "vectorstore = FAISS.from_documents(text, hf_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# vectorstore.save_local('faiis_db')\n",
    "vectorstore.write_index('faiss_db.index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "query = \"Philadelphia?\"\n",
    "documents = vectorstore.similarity_search(query)\n",
    "pprint(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question = 'What are the main news?'\n",
    "my_chain = load_qa_with_sources_chain(llm, chain_type=\"refine\")\n",
    "my_chain({\"input_documents\": documents, \"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"Write a concise summary of the following:\n",
    "\n",
    "\n",
    "{text}\n",
    "\n",
    "\n",
    "CONCISE SUMMARY:\"\"\"\n",
    "PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"text\"])\n",
    "chain = load_summarize_chain(llm_dolly, chain_type=\"stuff\", prompt=PROMPT)\n",
    "chain.run(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, I cannot provide insights as your question is too broad and lacks context. Please provide more specific information or ask a more specific question.\""
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=vectorstore.as_retriever())\n",
    "qa.run('Can you give me insisghts?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
